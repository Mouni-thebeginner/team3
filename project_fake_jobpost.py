# -*- coding: utf-8 -*-
"""project_fake_jobpost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rzwkpIp1-c5zFyVn298z5JKFwowg3zSR
"""

from google.colab import files
uploaded = files.upload()

# Install and import libraries
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Download NLTK resources (only need to run once in Colab)
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("stopwords")

# Load dataset (make sure the CSV is uploaded in Colab)
df = pd.read_csv("fake_job_postings.csv", encoding="latin-1")

# Function to preprocess text
def preprocess_text(text):
    if pd.isna(text):  # If cell is empty, return empty string
        return ""

    # Lowercase
    text = text.lower()

    # Tokenize
    tokens = word_tokenize(text)

    # Keep only alphabetic words
    tokens = [w for w in tokens if w.isalpha()]

    # Remove stopwords
    stop_words = set(stopwords.words("english"))
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    lems = [lemmatizer.lemmatize(word) for word in tokens]

    # Return as a single string
    return " ".join(lems)

# Identify all text (object) columns in the dataset
text_columns = df.select_dtypes(include="object").columns

# Apply preprocessing to each text column and create new "_clean" columns
for col in text_columns:
    df[col + "_clean"] = df[col].apply(preprocess_text)

# Show first few rows with cleaned text
print(df.head()[[col for col in df.columns if "_clean" in col]])

# Save final dataset with cleaned text
df.to_csv("preprocessed_fake_job_postings.csv", index=False, encoding="utf-8")
print("âœ… Preprocessing complete! File saved as preprocessed_fake_job_postings.csv")